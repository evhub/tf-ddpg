import gym
import numpy as np
import tensorflow as tf
from tqdm import tqdm

from ddpg.models import Actor, Critic
from ddpg.memory import ReplayMemory
from ddpg.util import run_with_sess, ornstein_uhlenbeck_noise


@run_with_sess
def train_with(sess, env, actor, critic, noise, num_episodes, batch_size, memory_size=100000, gamma=0.99, debug=False):
    """Train on an existing environment, actor, critic, and noise."""
    [obs_dim] = env.observation_space.shape
    [act_dim] = env.action_space.shape

    actor.update_target(sess)
    critic.update_target(sess)

    memory = ReplayMemory(memory_size)

    for episode_num in tqdm(range(num_episodes)):
        episode_rewards = []
        obs = env.reset()
        done = False
        while not done:

            [action] = actor.predict(sess, np.asarray([obs])) + next(noise)
            next_obs, reward, done, info = env.step(action)

            memory.add(obs, action, reward, done, next_obs)

            if len(memory) >= batch_size:
                if debug and len(memory) == batch_size:
                    print("\nFinished accumulating memory, starting training.")

                obs_batch, act_batch, r_batch, done_batch, next_obs_batch = memory.sample(batch_size)

                best_next_act_batch = actor.target_predict(sess, next_obs_batch)
                best_next_Q_batch = critic.target_predict(sess, next_obs_batch, best_next_act_batch)

                target_Q_values = np.asarray([
                    r if done else r + gamma * Q
                    for r, done, Q in zip(r_batch, done_batch, best_next_Q_batch)
                ])
                target_Q_batch = np.reshape(target_Q_values, (batch_size, 1))

                predicted_Q_values = critic.train(sess, obs_batch, act_batch, target_Q_batch)

                best_acts = actor.predict(sess, obs_batch)
                [Q_grads] = critic.get_Q_grads(sess, obs_batch, best_acts)
                actor.train(sess, obs_batch, Q_grads)

                actor.update_target(sess)
                critic.update_target(sess)

            obs = next_obs
            episode_rewards.append(reward)

        if debug:
            sum_disc_r = 0
            for i, r in enumerate(episode_rewards):
                sum_disc_r += gamma**i * r
            print(f"\nR_{episode_num} = {sum_disc_r} (over {len(episode_rewards)} steps)")

    return actor, critic, memory


def train(env_id, num_episodes, batch_size, *args, **kwargs):
    """Train a DDPG model on the given environment."""
    env = gym.make(env_id)

    [obs_dim] = env.observation_space.shape
    [act_dim] = env.action_space.shape

    actor = Actor(obs_dim, act_dim, batch_size)
    critic = Critic(obs_dim, act_dim)
    noise = ornstein_uhlenbeck_noise(np.zeros(act_dim))

    return train_with(env, actor, critic, noise, num_episodes, batch_size, *args, **kwargs)


if __name__ == "__main__":
    train("Pendulum-v0", num_episodes=100, batch_size=16, debug=True)
