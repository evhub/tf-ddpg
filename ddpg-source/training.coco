import gym
import numpy as np
from tqdm import tqdm

from ddpg.constants import (
    MEMORY_SIZE,
    DISCOUNT_RATE,
    TRAINING_EPISODES,
    BATCH_SIZE,
)
from ddpg.util import (
    run_with_sess,
    ornstein_uhlenbeck_noise,
)
from ddpg.memory import ReplayMemory
from ddpg.models import Actor, Critic


@run_with_sess
def train_with(
    sess,
    env,
    actor,
    critic,
    noise,
    num_episodes=TRAINING_EPISODES,
    batch_size=BATCH_SIZE,
    memory_size=MEMORY_SIZE,
    gamma=DISCOUNT_RATE,
    debug=False,
):
    """Train on an existing environment, actor, critic, and noise."""
    [obs_dim] = env.observation_space.shape
    [act_dim] = env.action_space.shape

    actor.update_target(sess)
    critic.update_target(sess)

    memory = ReplayMemory(memory_size)

    for episode_num in tqdm(range(num_episodes)):
        episode_rewards = []

        obs = env.reset()

        done = False
        while not done:

            [action] = actor.predict(sess, np.asarray([obs])) + next(noise)
            next_obs, reward, done, info = env.step(action)

            memory.add(obs, action, reward, done)

            if len(memory) >= batch_size:
                if debug and len(memory) == batch_size:
                    print("\nFinished accumulating memory; starting training.")

                obs_batch, act_batch, r_batch, done_batch, next_obs_batch = memory.sample(batch_size)

                best_next_act_batch = actor.target_predict(sess, next_obs_batch)
                best_next_Q_batch = critic.target_predict(sess, next_obs_batch, best_next_act_batch)

                target_Q_values = np.asarray([
                    r if done else r + gamma * Q
                    for r, done, Q in zip(r_batch, done_batch, best_next_Q_batch)
                ])
                target_Q_batch = np.reshape(target_Q_values, (batch_size, 1))

                critic.train(sess, obs_batch, act_batch, target_Q_batch)

                actor.train(sess, obs_batch)

                critic.update_target(sess)
                actor.update_target(sess)

            obs = next_obs
            episode_rewards.append(reward)

        if debug:
            sum_disc_r = 0
            for i, r in enumerate(episode_rewards):
                sum_disc_r += gamma**i * r
            print(f"\nR_{episode_num} = {sum_disc_r} (over {len(episode_rewards)} steps)")

    return actor, critic, memory


def train(env_id, *args, **kwargs):
    """Train a DDPG model on the given environment."""
    env = gym.make(env_id)

    [obs_dim] = env.observation_space.shape
    [act_dim] = env.action_space.shape

    # tf global variables are created here so they will be initialized by
    #  @run_with_sess when we call train_with
    critic = Critic(obs_dim, act_dim)
    actor = Actor(obs_dim, act_dim, critic)

    noise = ornstein_uhlenbeck_noise(np.zeros(act_dim))

    return train_with(env, actor, critic, noise, *args, **kwargs)


if __name__ == "__main__":
    train("Pendulum-v0", debug=True)
