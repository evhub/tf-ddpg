import tensorflow as tf

from ddpg.util import (
    batch_input,
    get_params_defined_in,
    get_target_model_updater,
    run_sess_with_opt,
)
from ddpg.networks import get_actor, get_critic


def get_actor_with_params(obs_input, act_dim) =
    """Return (actor_params, actor_model)."""
    get_params_defined_in(get_actor$(obs_input, act_dim))


def get_critic_with_params(obs_input, act_input) =
    """Return (critic_params, critic_model)."""
    get_params_defined_in(get_critic$(obs_input, act_input))


class Critic:
    """A DDPG critic. Keeps track of both the current critic and the target critic."""

    def __init__(self, obs_dim, act_dim):
        # build critic model
        self.obs_input = batch_input(obs_dim)
        self.act_input = batch_input(act_dim)
        self.params, self.critic = get_critic_with_params(self.obs_input, self.act_input)
        self.target_params, self.target_critic = get_critic_with_params(self.obs_input, self.act_input)
        self.target_updater = get_target_model_updater(self.target_params, self.params)

        # construct MSE loss
        self.target_Q_input = batch_input(1)
        self.optimizer = tf.train.AdamOptimizer().minimize(
            tf.losses.mean_squared_error(self.target_Q_input, self.critic),
            var_list=self.params,
        )

    def train(self, sess, obs_batch, act_batch, target_Q_batch) =
        """Train the critic on the given observation, action, and target Q value batches."""
        run_sess_with_opt(sess, self.optimizer, [self.critic], feed_dict={
            self.obs_input: obs_batch,
            self.act_input: act_batch,
            self.target_Q_input: target_Q_batch,
        })

    def predict(self, sess, obs_batch, act_batch) =
        """Get the predicted Q value for the given observation and action batches."""
        sess.run(self.critic, feed_dict={
            self.obs_input: obs_batch,
            self.act_input: act_batch,
        })

    def target_predict(self, sess, obs_batch, act_batch) =
        """Same as predict but uses the target critic."""
        sess.run(self.target_critic, feed_dict={
            self.obs_input: obs_batch,
            self.act_input: act_batch,
        })

    def update_target(self, sess):
        """Update the target critic."""
        sess.run(self.target_updater)


class Actor:
    """A DDPG actor. Keeps track of both the current actor and the target actor."""

    def __init__(self, obs_dim, act_dim, critic):
        # build actor model
        self.obs_input = batch_input(obs_dim)
        self.params, self.actor = get_actor_with_params(self.obs_input, act_dim)
        self.target_params, self.target_actor = get_actor_with_params(self.obs_input, act_dim)
        self.target_updater = get_target_model_updater(self.target_params, self.params)

        # use critic to construct loss
        self.critic_params, self.self_critic = get_critic_with_params(self.obs_input, self.actor)
        self.critic_updater = get_target_model_updater(self.critic_params, critic.params, update_weight=1)
        self.optimizer = tf.train.AdamOptimizer().minimize(
            -self.self_critic,
            var_list=self.params,
        )

    def train(self, sess, obs_batch) =
        """Train the actor on the given batch of observations."""
        run_sess_with_opt(sess, self.optimizer, [self.actor], feed_dict={
            self.obs_input: obs_batch,
        })

    def predict(self, sess, obs_batch) =
        """Get the predicted best actions for the given observations."""
        sess.run(self.actor, feed_dict={
            self.obs_input: obs_batch,
        })

    def target_predict(self, sess, obs_batch) =
        """Same as predict but uses the target actor."""
        sess.run(self.target_actor, feed_dict={
            self.obs_input: obs_batch,
        })

    def update_target(self, sess):
        """Update the target actor."""
        sess.run(self.target_updater)
        sess.run(self.critic_updater)
